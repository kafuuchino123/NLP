import os
import re
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import numpy as np
from tqdm import tqdm

from PyPDF2 import PdfReader
import nltk
from sentence_transformers import SentenceTransformer
import faiss

# ä¸‹è½½å¿…è¦çš„ NLTK æ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

@dataclass
class IndexMetadata:
    """ç´¢å¼•å…ƒæ•°æ®"""
    file_path: str
    file_name: str
    created_at: str
    chunk_size: int
    overlap: int
    total_chunks: int
    embedding_model: str
    file_hash: str

def compute_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶çš„ SHA-256 å“ˆå¸Œå€¼"""
    import hashlib
    with open(file_path, "rb") as f:
        return hashlib.sha256(f.read()).hexdigest()[:16]

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºç™½"""
    # æ›¿æ¢å¸¸è§çš„ç‰¹æ®Šå­—ç¬¦ä¸ºç©ºæ ¼
    text = re.sub(r'[_*#\[\]()~>`]', ' ', text)
    # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_document(file_path: str) -> str:
    """è¯»å– PDF æˆ– Markdown æ–‡æ¡£ï¼Œè¿”å›æ¸…ç†åçš„çº¯æ–‡æœ¬"""
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        
    try:
        if file_path.suffix.lower() == '.pdf':
            reader = PdfReader(file_path)
            text = []
            for page in tqdm(reader.pages, desc="è¯»å– PDF"):
                content = page.extract_text() or ""
                text.append(clean_text(content))
            return "\n".join(text)
            
        elif file_path.suffix.lower() in ['.md', '.markdown']:
            with open(file_path, "r", encoding="utf-8") as f:
                return clean_text(f.read())
        else:
            raise ValueError("åªæ”¯æŒ PDF å’Œ Markdown æ–‡ä»¶")
            
    except Exception as e:
        raise RuntimeError(f"è¯»å–æ–‡ä»¶ {file_path.name} å¤±è´¥: {str(e)}") from e


# æ¨¡å‹ç¼“å­˜ï¼Œé¿å…å¤šæ¬¡åŠ è½½
_sbert_model = None

def get_sbert_model(model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
    global _sbert_model
    if _sbert_model is None:
        _sbert_model = SentenceTransformer(model_name)
    return _sbert_model

def split_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """æ™ºèƒ½åˆ‡åˆ†æ–‡æ¡£ä¸ºå¥å­å—
    
    ä½¿ç”¨ NLTK è¿›è¡Œå¥å­åˆ†å‰²ï¼Œç„¶åå°†å¥å­ç»„åˆæˆé€‚å½“å¤§å°çš„å—ï¼Œ
    ç¡®ä¿ä¸ä¼šåœ¨å¥å­ä¸­é—´æˆªæ–­ã€‚
    """
    # åˆ†å¥
    sentences = nltk.sent_tokenize(text)
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        sentence_length = len(sentence)
        
        # å¦‚æœå•ä¸ªå¥å­è¶…è¿‡å—å¤§å°ï¼ŒæŒ‰è¯åˆ‡åˆ†
        if sentence_length > chunk_size:
            words = sentence.split()
            temp_chunk = []
            temp_length = 0
            
            for word in words:
                word_length = len(word) + 1  # +1 for space
                if temp_length + word_length > chunk_size:
                    if temp_chunk:
                        chunks.append(" ".join(temp_chunk))
                    temp_chunk = [word]
                    temp_length = word_length
                else:
                    temp_chunk.append(word)
                    temp_length += word_length
                    
            if temp_chunk:
                chunks.append(" ".join(temp_chunk))
            continue
            
        # æ­£å¸¸å¥å­å¤„ç†
        if current_length + sentence_length + 1 <= chunk_size:
            current_chunk.append(sentence)
            current_length += sentence_length + 1
        else:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length
            
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(" ".join(current_chunk))
        
    # å¤„ç†é‡å 
    if overlap > 0 and len(chunks) > 1:
        overlapped_chunks = []
        for i in range(len(chunks)):
            if i > 0:
                # ä»å‰ä¸€ä¸ªå—çš„æœ«å°¾è·å–é‡å éƒ¨åˆ†
                prev_words = chunks[i-1].split()[-overlap//10:]  # é‡å è¯æ•°è€Œä¸æ˜¯å­—ç¬¦
                current_chunk = " ".join(prev_words) + " " + chunks[i]
                overlapped_chunks.append(current_chunk)
            else:
                overlapped_chunks.append(chunks[i])
        chunks = overlapped_chunks
        
    return chunks

def build_index(
    file_path: str,
    index_dir: str = "index",
    chunk_size: int = 500,
    overlap: int = 50,
    batch_size: int = 32
) -> IndexMetadata:
    """æ„å»º FAISS ç´¢å¼•
    
    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        index_dir: ç´¢å¼•å­˜å‚¨ç›®å½•
        chunk_size: æ–‡æœ¬å—å¤§å°
        overlap: å—é—´é‡å é•¿åº¦
        batch_size: å‘é‡åŒ–æ‰¹æ¬¡å¤§å°
    
    Returns:
        IndexMetadata: ç´¢å¼•å…ƒæ•°æ®
    """
    index_dir = Path(index_dir)
    index_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. åŠ è½½å’Œå¤„ç†æ–‡æ¡£
        print("ğŸ“š æ­£åœ¨è¯»å–æ–‡æ¡£...")
        text = load_document(file_path)
        
        # 2. æ–‡æœ¬åˆ†å—
        print("âœ‚ï¸ æ­£åœ¨æ™ºèƒ½åˆ†å—...")
        chunks = split_text(text, chunk_size=chunk_size, overlap=overlap)
        if not chunks:
            raise ValueError("æ–‡æ¡£å¤„ç†åæ²¡æœ‰æœ‰æ•ˆå†…å®¹")
        
        # 3. å‘é‡åŒ–ï¼ˆæ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
        print("ğŸ”¢ æ­£åœ¨ç”Ÿæˆå‘é‡è¡¨ç¤º...")
        model = get_sbert_model()
        embeddings_list = []
        
        for i in tqdm(range(0, len(chunks), batch_size), desc="å‘é‡åŒ–è¿›åº¦"):
            batch = chunks[i:i + batch_size]
            batch_embeddings = model.encode(
                batch,
                convert_to_numpy=True,
                show_progress_bar=False
            )
            embeddings_list.append(batch_embeddings)
            
        embeddings = np.vstack(embeddings_list)
        
        # 4. æ„å»º FAISS ç´¢å¼•
        print("ğŸ” æ­£åœ¨æ„å»ºæ£€ç´¢ç´¢å¼•...")
        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)
        index.add(embeddings)
        
        # 5. ä¿å­˜ç´¢å¼•å’Œå—æ•°æ®
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•æ–‡ä»¶...")
        
        # ç”Ÿæˆå…ƒæ•°æ®
        metadata = IndexMetadata(
            file_path=str(file_path),
            file_name=Path(file_path).name,
            created_at=datetime.now().isoformat(),
            chunk_size=chunk_size,
            overlap=overlap,
            total_chunks=len(chunks),
            embedding_model=model._model_name if hasattr(model, '_model_name') else "sentence-transformers/all-MiniLM-L6-v2",
            file_hash=compute_file_hash(file_path)
        )
        
        # ä¿å­˜æ‰€æœ‰æ–‡ä»¶
        faiss.write_index(index, str(index_dir / "faiss.index"))
        
        with open(index_dir / "chunks.txt", "w", encoding="utf-8") as f:
            for chunk in chunks:
                f.write(chunk.replace("\n", " ") + "\n")
                
        with open(index_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(asdict(metadata), f, ensure_ascii=False, indent=2)
            
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼å…±å¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return metadata
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{str(e)}")
        # æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†æ–‡ä»¶
        for file in ["faiss.index", "chunks.txt", "metadata.json"]:
            try:
                (index_dir / file).unlink(missing_ok=True)
            except Exception:
                pass
        raise
